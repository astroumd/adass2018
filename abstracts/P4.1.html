<html> <body>
Prev: <A HREF=O4.5.html>O4.5</A> Next: <A HREF=P4.2.html>P4.2</A> <br><br>
<b>P4.1: Albert, Kinga</b>
<br>
K. Albert (Max Planck Institute for Solar System Research (MPS)) <br> J. Hizberger (Max Planck Institute for Solar System Research (MPS)) <br>  D. Busse (Max Planck Institute for Solar System Research (MPS))<br>  J. Blanco Rodríguez (Universidad de Valencia)<br>  J. P. Cobos Carrascosa (Instituto de Astrofisica de Andalucía (IAA - CSIC))<br>  B. Fiethe, Institute of Computer and Network Engineering at the TU Braunschweig (IDA), A. Gandorfer, MPS, Y. Guan, IDA, M. Kolleck, MPS, T. Lange, IDA, H. Michalik, IDA, S. K. Solanki, MPS, J. C. del Toro Iniesta, IAA - CSIC, and J. Woch (MPS)<br><br>
<br>
<b>Theme:</b>  Data Science: Workflows Hardware Software Humanware
<br>
<b>Title:</b> <i>Performance analysis of the SO/PHI software framework for on-board data reduction</i>
<br><br>
The Polarimetric and Helioseismic Imager (PHI) is the first solar spectropolarimeter that goes to deep space. It will be launched on-board the Solar Orbiter (SO) spacecraft, to orbit the Sun in highly elliptical orbits. SO/PHI has stringent requirements on its science data accuracy, while it is subjected to highly dynamic environments by the spacecraft. The orbits, however, severely limit the amount of telemetry, making the download of numerous full datasets and additional calibration data unfeasible. In addition, they also increase the command-response times of the instrument.
To overcome these limitations, SO/PHI implements autonomous on-board instrument calibration, and autonomous on-board science data reduction. The algorithms are implemented within a system of software framework and dedicated hardware implementations in reconfigurable FPGA-s. The system is designed to overcome the resource and computational power limitations of on-board computing, do secure metadata logging and warn ground support of possible errors in the data processing flow.
In this contribution we do an in-depth performance analysis of the on-board data processing system. We test multiple data pipelines on realistic data in a step by step analysis, and tune the pipeline parameters to achieve optimal results. We finally analyse the accuracy of the resulting scientific data products calculated by the instrument.
</body> </html>
